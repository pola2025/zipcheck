/**
 * ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
 * ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ ì „ í˜„ì¬ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ë°±ì—…í•©ë‹ˆë‹¤.
 */

import { supabase } from '../lib/supabase'
import * as fs from 'fs'
import * as path from 'path'

interface BackupResult {
	timestamp: string
	tables: {
		quote_requests: any[]
		quote_groups: any[]
		// ì¶”ê°€ í…Œì´ë¸”ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€
	}
	metadata: {
		totalRecords: number
		backupSize: string
	}
}

async function backupDatabase() {
	console.log('ğŸ”„ Starting database backup...\n')

	const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
	const backupDir = path.resolve(__dirname, '../../../backups')
	const backupFile = path.join(backupDir, `backup-${timestamp}.json`)

	try {
		// ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
		if (!fs.existsSync(backupDir)) {
			fs.mkdirSync(backupDir, { recursive: true })
			console.log('ğŸ“ Created backup directory:', backupDir)
		}

		const backup: BackupResult = {
			timestamp,
			tables: {
				quote_requests: [],
				quote_groups: []
			},
			metadata: {
				totalRecords: 0,
				backupSize: ''
			}
		}

		// 1. quote_requests ë°±ì—…
		console.log('â³ Backing up quote_requests table...')
		const { data: quoteRequests, error: qrError } = await supabase
			.from('quote_requests')
			.select('*')
			.order('created_at', { ascending: false })

		if (qrError) {
			console.error('âŒ Error backing up quote_requests:', qrError.message)
		} else {
			backup.tables.quote_requests = quoteRequests || []
			console.log(`âœ… Backed up ${backup.tables.quote_requests.length} quote_requests`)
		}

		// 2. quote_groups ë°±ì—…
		console.log('â³ Backing up quote_groups table...')
		const { data: quoteGroups, error: qgError } = await supabase
			.from('quote_groups')
			.select('*')
			.order('created_at', { ascending: false })

		if (qgError) {
			// í…Œì´ë¸”ì´ ì—†ì„ ìˆ˜ë„ ìˆìŒ (ì •ìƒ)
			if (qgError.message.includes('does not exist')) {
				console.log('â„¹ï¸  quote_groups table does not exist (OK)')
			} else {
				console.error('âŒ Error backing up quote_groups:', qgError.message)
			}
		} else {
			backup.tables.quote_groups = quoteGroups || []
			console.log(`âœ… Backed up ${backup.tables.quote_groups.length} quote_groups`)
		}

		// ë©”íƒ€ë°ì´í„° ê³„ì‚°
		backup.metadata.totalRecords =
			backup.tables.quote_requests.length + backup.tables.quote_groups.length

		// JSON íŒŒì¼ë¡œ ì €ì¥
		const jsonContent = JSON.stringify(backup, null, 2)
		fs.writeFileSync(backupFile, jsonContent, 'utf-8')

		const fileSizeBytes = fs.statSync(backupFile).size
		const fileSizeKB = (fileSizeBytes / 1024).toFixed(2)
		backup.metadata.backupSize = `${fileSizeKB} KB`

		console.log('\n' + '='.repeat(60))
		console.log('âœ… Backup completed successfully!')
		console.log('='.repeat(60))
		console.log(`ğŸ“ Backup file: ${backupFile}`)
		console.log(`ğŸ“Š Total records: ${backup.metadata.totalRecords}`)
		console.log(`ğŸ’¾ File size: ${backup.metadata.backupSize}`)
		console.log('='.repeat(60))

		console.log('\nğŸ’¡ Backup file location:')
		console.log(`   ${backupFile}`)

		console.log('\nğŸ’¡ To restore from backup, use:')
		console.log(`   npm run restore:backup ${path.basename(backupFile)}`)

		return backupFile
	} catch (error) {
		console.error('\nâŒ Backup failed:', error)
		process.exit(1)
	}
}

// ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
backupDatabase()
